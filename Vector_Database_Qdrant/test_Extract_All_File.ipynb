{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Thực thi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Hàm đếm từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_simple(text):\n",
    "    # Đếm số tokens đơn giản dựa trên khoảng trắng\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Hàm trích xuất các từ khóa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    # Loại bỏ dấu câu ngoại trừ \"/\" và \"-\"\n",
    "    text = re.sub(r'[^\\w\\s/-]', '', text)\n",
    "    words = text.split()\n",
    "    unique_words = list(dict.fromkeys(words))  # Loại bỏ từ trùng lặp\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keywords(keywords):\n",
    "    result = []\n",
    "    \n",
    "    # Biểu thức chính quy để nhận diện số La Mã\n",
    "    roman_numeral_pattern = re.compile(r\"^(?=[MDCLXVI])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\")\n",
    "    \n",
    "    for word in keywords:\n",
    "        if roman_numeral_pattern.match(word) or any(c in word for c in \"/-\"):  # Giữ nguyên số La Mã hoặc chứa \"/\" hay \"-\"\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(word.lower())  # Chuyển các từ còn lại thành chữ thường\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Hàm xử lý tên file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_name(file_name):\n",
    "    # Giả sử số thứ tự nằm ở phần đầu tên file và được tách ra bằng dấu \"_\"\n",
    "    parts = file_name.replace('.docx', '').split('_')\n",
    "\n",
    "    if len(parts) != 8:\n",
    "        return None  # Nếu file không hợp lệ\n",
    "    \n",
    "    stt = parts[0]  # Số thứ tự\n",
    "    loai_van_ban = parts[1]\n",
    "    noi_ban_hanh = parts[2]\n",
    "    so_hieu = parts[3].replace(\"-\", \"/\")\n",
    "    linhvuc_nganh = parts[4]\n",
    "    ngay_ban_hanh = parts[5].replace(\"-\", \"/\")\n",
    "    ngay_hieu_luc = parts[6] if parts[6] == \"Đã biết\" else parts[6].replace(\"-\", \"/\")\n",
    "    chu_de = parts[7]\n",
    "    \n",
    "    # Tách keywords từ tên file và xử lý\n",
    "    metadata_keywords = extract_keywords(f\"{loai_van_ban} {noi_ban_hanh} {so_hieu} {linhvuc_nganh} {ngay_ban_hanh} {ngay_hieu_luc} {chu_de}\")\n",
    "    metadata_keywords = process_keywords(metadata_keywords)\n",
    "\n",
    "    return {\n",
    "        \"stt\": stt,  # Thêm số thứ tự\n",
    "        \"loai_van_ban\": loai_van_ban,\n",
    "        \"noi_ban_hanh\": noi_ban_hanh,\n",
    "        \"so_hieu\": so_hieu,\n",
    "        \"linhvuc_nganh\": linhvuc_nganh,\n",
    "        \"ngay_ban_hanh\": ngay_ban_hanh,\n",
    "        \"ngay_hieu_luc\": ngay_hieu_luc,\n",
    "        \"chu_de\": chu_de,\n",
    "        \"key_words\": metadata_keywords  # Lưu lại key words từ tên file\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Hàm đọc và xử lý nội dung của file .docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docx(file_path, file_metadata, max_tokens):\n",
    "    document = Document(file_path)\n",
    "\n",
    "    current_chapter = None\n",
    "    current_section = None\n",
    "    current_mini_section = None\n",
    "    data = []\n",
    "    \n",
    "    for i in range(len(document.paragraphs)):\n",
    "        paragraph_text = document.paragraphs[i].text.strip()\n",
    "\n",
    "        if paragraph_text.startswith(\"Chương\"):\n",
    "            current_chapter = paragraph_text\n",
    "            current_section = None\n",
    "            current_mini_section = None\n",
    "            if i + 1 < len(document.paragraphs):\n",
    "                next_paragraph = document.paragraphs[i + 1].text.strip()\n",
    "                if not next_paragraph.startswith((\"Mục\", \"Tiểu mục\", \"Điều\")):\n",
    "                    current_chapter += f\": {next_paragraph}\"\n",
    "            continue\n",
    "\n",
    "        if paragraph_text.startswith(\"Mục\"):\n",
    "            current_section = paragraph_text\n",
    "            current_mini_section = None\n",
    "            continue\n",
    "\n",
    "        if paragraph_text.startswith(\"Tiểu mục\"):\n",
    "            current_mini_section = paragraph_text\n",
    "            continue\n",
    "\n",
    "        if paragraph_text.startswith(\"Điều\"):\n",
    "            article = paragraph_text\n",
    "            content = []\n",
    "            article_sections = []  # Lưu nội dung các Khoản\n",
    "\n",
    "            for j in range(i + 1, len(document.paragraphs)):\n",
    "                next_paragraph = document.paragraphs[j].text.strip()\n",
    "                if next_paragraph.startswith((\"Chương\", \"Mục\", \"Tiểu mục\", \"Điều\")):\n",
    "                    break\n",
    "                content.append(next_paragraph)\n",
    "\n",
    "                # Kiểm tra xem đoạn văn có phải là Khoản không\n",
    "                if re.match(r'^\\d+\\.', next_paragraph):\n",
    "                    article_sections.append(next_paragraph)\n",
    "\n",
    "            # Lưu trữ nội dung không phải Khoản\n",
    "            content_text = \"\\n\".join(content)\n",
    "            \n",
    "            # Tách từ khóa từ nội dung và gộp với từ khóa từ file_name\n",
    "            content_key_words = process_keywords(extract_keywords(f\"{article}\\n{content_text}\"))\n",
    "            \n",
    "            # Gộp từ khóa từ file name và nội dung rồi loại bỏ trùng lặp\n",
    "            all_key_words = list(dict.fromkeys(file_metadata[\"key_words\"] + content_key_words))\n",
    "\n",
    "            # Tạo thêm từ khóa mới nếu có ký tự \"/\" trong từ khóa\n",
    "            for kw in all_key_words[:]:  # Sử dụng [:] để sao chép danh sách trong vòng lặp\n",
    "                if \"/\" in kw:\n",
    "                    new_kw = kw.replace(\"/\", \"-\")\n",
    "                    if new_kw not in all_key_words:  # Kiểm tra xem từ khóa mới đã có chưa\n",
    "                        all_key_words.append(new_kw)\n",
    "\n",
    "            # Kiểm tra số lượng tokens\n",
    "            if count_tokens_simple(content_text) > max_tokens - count_tokens_simple(article):\n",
    "                numbered_sections = re.split(r'(?=\\d+\\.\\s)', content_text)\n",
    "\n",
    "                current_content = \"\"\n",
    "                for section in numbered_sections:\n",
    "                    if count_tokens_simple(current_content + section) <= max_tokens - count_tokens_simple(article):\n",
    "                        current_content += section + \"\\n\"\n",
    "                    else:\n",
    "                        data.append({\n",
    "                            \"Chapter\": current_chapter,\n",
    "                            \"Section\": current_section if current_section else None,\n",
    "                            \"Mini-Section\": current_mini_section if current_mini_section else None,\n",
    "                            \"Article\": article,\n",
    "                            \"Content\": current_content.strip(),\n",
    "                            \"Article-Section\": f\"Khoản {len(article_sections) + 1}.\",  # Lưu số thứ tự Khoản\n",
    "                            \"combine_Article_Content\": f\"{article}\\n{current_content.strip()}\",\n",
    "                            \"key_words\": all_key_words\n",
    "                        })\n",
    "                        current_content = section + \"\\n\"\n",
    "\n",
    "                if current_content:\n",
    "                    data.append({\n",
    "                        \"Chapter\": current_chapter,\n",
    "                        \"Section\": current_section if current_section else None,\n",
    "                        \"Mini-Section\": current_mini_section if current_mini_section else None,\n",
    "                        \"Article\": article,\n",
    "                        \"Content\": current_content.strip(),\n",
    "                        \"Article-Section\": f\"Khoản {len(article_sections) + 1}.\",  # Lưu số thứ tự Khoản\n",
    "                        \"combine_Article_Content\": f\"{article}\\n{current_content.strip()}\",\n",
    "                        \"key_words\": all_key_words\n",
    "                    })\n",
    "            else:\n",
    "                # Nếu không có Khoản\n",
    "                if article_sections:\n",
    "                    for section in article_sections:\n",
    "                        data.append({\n",
    "                            \"Chapter\": current_chapter,\n",
    "                            \"Section\": current_section if current_section else None,\n",
    "                            \"Mini-Section\": current_mini_section if current_mini_section else None,\n",
    "                            \"Article\": article,\n",
    "                            \"Content\": section.strip(),\n",
    "                            \"Article-Section\": section.split('.')[0] + '.',  # Lưu số thứ tự Khoản\n",
    "                            \"combine_Article_Content\": f\"{article}\\n{section.strip()}\",\n",
    "                            \"key_words\": all_key_words\n",
    "                        })\n",
    "                else:\n",
    "                    data.append({\n",
    "                        \"Chapter\": current_chapter,\n",
    "                        \"Section\": current_section if current_section else None,\n",
    "                        \"Mini-Section\": current_mini_section if current_mini_section else None,\n",
    "                        \"Article\": article,\n",
    "                        \"Content\": content_text.strip(),\n",
    "                        \"Article-Section\": None,\n",
    "                        \"combine_Article_Content\": f\"{article}\\n{content_text.strip()}\",\n",
    "                        \"key_words\": all_key_words\n",
    "                    })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Hàm chính xử lý tất cả các file trong thư mục"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_path, output_json_path, max_tokens):\n",
    "    all_data = []\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".docx\"):\n",
    "            file_metadata = process_file_name(file_name)\n",
    "            if file_metadata is None:\n",
    "                continue\n",
    "            \n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            docx_data = process_docx(file_path, file_metadata, max_tokens=max_tokens)\n",
    "            \n",
    "            for entry in docx_data:\n",
    "                combined_entry = {**file_metadata, **entry}\n",
    "                all_data.append(combined_entry)\n",
    "    \n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(all_data, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Dữ liệu đã được lưu vào file {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Thực thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn tới thư mục và file đầu ra\n",
    "folder_path = r'data\\trich_dan_luat\\docx\\Luat'\n",
    "json_file_path = r'data\\trich_dan_luat\\json\\test_Combined_Output_Token_Keywords.json'\n",
    "max_tokens = 2048  # Giới hạn số token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được lưu vào file data\\trich_dan_luat\\json\\test_Combined_Output_Token_Keywords.json\n"
     ]
    }
   ],
   "source": [
    "# Chạy chương trình chính\n",
    "process_folder(folder_path, json_file_path, max_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
