{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaForQuestionAnswering, Trainer, TrainingArguments,RobertaTokenizerFast\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> data origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bh=pd.read_csv(\"../data/data_tinhhuong/csv/bhxh_bhyt_bhtn_data.csv\")\n",
    "df_chinhsach=pd.read_csv(\"../data/data_tinhhuong/csv/chinhsach_nguoicocong_data.csv\")\n",
    "df_gd_daotao_yte=pd.read_csv(\"../data/data_tinhhuong/csv/giaoduc_daotao_yte_data.csv\")\n",
    "df_gt_xd_tn_mt=pd.read_csv(\"../data/data_tinhhuong/csv/giaothong_xaydung_tainguyen_moitruong_data.csv\")\n",
    "df_laodong_tt=pd.read_csv(\"../data/data_tinhhuong/csv/laodong_tienthuong_data.csv\")\n",
    "df_tc_nh_dt_ct=pd.read_csv(\"../data/data_tinhhuong/csv/taichinh_nganhang_dautu_congthuong_data.csv\")\n",
    "df_linhvuckhac=pd.read_csv(\"../data/data_tinhhuong/csv/linhvuckhac_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ná»‘i cÃ¡c DataFrame láº¡i thÃ nh má»™t DataFrame duy nháº¥t\n",
    "df_tonghop = pd.concat([df_bh, df_chinhsach, df_gd_daotao_yte, df_gt_xd_tn_mt, df_laodong_tt, df_tc_nh_dt_ct, df_linhvuckhac], ignore_index=True)\n",
    "\n",
    "df_tonghop = df_tonghop.drop(columns='Index')\n",
    "\n",
    "# LÆ°u DataFrame Ä‘Ã£ ná»‘i vá»›i viá»‡c thÃªm cá»™t 'index' lÃ m tÃªn cá»™t\n",
    "df_tonghop.to_csv(\"../data/data_tinhhuong/csv/tonghop_data.csv\", index_label='Index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sumary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12818 entries, 0 to 12817\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Name       12816 non-null  object\n",
      " 1   Time       12818 non-null  object\n",
      " 2   Question   12818 non-null  object\n",
      " 3   Situation  12818 non-null  object\n",
      " 4   Answer     12818 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 500.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_tonghop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Index', 'Name', 'Time', 'Question', 'Situation', 'Answer'], dtype='object')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tonghop=pd.read_csv(\"../data/data_tinhhuong/csv/tonghop_data.csv\")\n",
    "df_tonghop.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chuáº©n bá»‹ dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'context' and 'answers' columns\n",
    "df_tonghop['context'] = df_tonghop['Question'] + \" \" + df_tonghop['Situation']\n",
    "df_tonghop['answers'] = df_tonghop['Answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_tonghop[['Question', 'context', 'answers']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': ['NgÆ°á»i dÃ¢n tá»™c thiá»ƒu sá»‘ xÃ£ nÃ´ng thÃ´n má»›i cÃ³ Ä‘Æ°á»£c há»— trá»£ BHYT?',\n",
       "  'CÃ³ Ä‘Æ°á»£c truy lÄ©nh cháº¿ Ä‘á»™ thai sáº£n?',\n",
       "  'Khi nÃ o cáº§n xin thÃªm Giáº¥y chá»©ng nháº­n nghá»‰ viá»‡c hÆ°á»Ÿng BHXH?',\n",
       "  'ÄÃ³ng BHXH tá»± nguyá»‡n bao lÃ¢u thÃ¬ Ä‘Æ°á»£c lÆ°Æ¡ng hÆ°u?',\n",
       "  'ÄÃ³ng BHXH bao lÃ¢u trÆ°á»›c khi sinh thÃ¬ Ä‘Æ°á»£c hÆ°á»Ÿng cháº¿ Ä‘á»™ thai sáº£n?'],\n",
       " 'context': ['NgÆ°á»i dÃ¢n tá»™c thiá»ƒu sá»‘ xÃ£ nÃ´ng thÃ´n má»›i cÃ³ Ä‘Æ°á»£c há»— trá»£ BHYT? TÃ´i xin há»i, má»™t xÃ£ giai Ä‘oáº¡n 2016-2020 lÃ  xÃ£ khu vá»±c III, sang giai Ä‘oáº¡n 2021-2025 lÃ  xÃ£ khu vá»±c I, Ä‘Ã£ Ä‘áº¡t chuáº©n xÃ£ nÃ´ng thÃ´n má»›i thÃ¬ ngÆ°á»i dÃ¢n tá»™c thiá»ƒu sá»‘ trÃªn Ä‘á»‹a bÃ n xÃ£ nÃ y cÃ³ thuá»™c nhÃ³m Ä‘á»‘i tÆ°á»£ng Ä‘Æ°á»£c ngÃ¢n sÃ¡ch NhÃ  nÆ°á»›c há»— trá»£ Ä‘Ã³ng BHYT khÃ´ng?',\n",
       "  'CÃ³ Ä‘Æ°á»£c truy lÄ©nh cháº¿ Ä‘á»™ thai sáº£n? TÃ´i Ä‘Ã³ng BHXH tá»« thÃ¡ng 6/2011 Ä‘áº¿n thÃ¡ng 12/2014, thÃ¡ng 8/2014 tÃ´i sinh con. Do cÃ´ng ty ná»£ tiá»n BHXH nÃªn háº¿t thá»i gian nghá»‰ thai sáº£n tÃ´i chÆ°a Ä‘Æ°á»£c thanh toÃ¡n cháº¿ Ä‘á»™. NÄƒm 2021 cÃ´ng ty Ä‘Ã£ ná»™p háº¿t sá»‘ tiá»n ná»£ BHXH vÃ  giáº£i thá»ƒ. Xin há»i, bÃ¢y giá» tÃ´i lÃ m Ä‘Æ¡n xin hÆ°á»Ÿng cháº¿ Ä‘á»™ thai sáº£n cá»§a nÄƒm 2014 cÃ³ Ä‘Æ°á»£c khÃ´ng?',\n",
       "  'Khi nÃ o cáº§n xin thÃªm Giáº¥y chá»©ng nháº­n nghá»‰ viá»‡c hÆ°á»Ÿng BHXH? TÃ´i xin há»i, trÃªn giáº¥y ra viá»‡n cÃ³ ghi chÃº ngÆ°á»i lao Ä‘á»™ng Ä‘Æ°á»£c nghá»‰ lao Ä‘á»™ng 10 ngÃ y, váº­y, cÃ³ cáº§n xin thÃªm Giáº¥y chá»©ng nháº­n nghá»‰ viá»‡c hÆ°á»Ÿng BHXH Ä‘á»ƒ hÆ°á»Ÿng 10 ngÃ y Ä‘Ã³ khÃ´ng?',\n",
       "  'ÄÃ³ng BHXH tá»± nguyá»‡n bao lÃ¢u thÃ¬ Ä‘Æ°á»£c lÆ°Æ¡ng hÆ°u? TÃ´i nÄƒm nay 53 tuá»•i, Ä‘Ã³ng BHXH tá»± nguyá»‡n Ä‘Æ°á»£c 5 nÄƒm. Xin há»i, tÃ´i tiáº¿p tá»¥c Ä‘Ã³ng BHXH tá»± nguyá»‡n Ä‘áº¿n nÄƒm 60 tuá»•i thÃ¬ cÃ³ Ä‘Æ°á»£c nháº­n lÆ°Æ¡ng hÆ°u khÃ´ng?',\n",
       "  'ÄÃ³ng BHXH bao lÃ¢u trÆ°á»›c khi sinh thÃ¬ Ä‘Æ°á»£c hÆ°á»Ÿng cháº¿ Ä‘á»™ thai sáº£n? TÃ´i Ä‘Ã³ng BHXH tá»« thÃ¡ng 1/2020 Ä‘áº¿n thÃ¡ng 12/2023. TÃ´i Ä‘Ã£ nghá»‰ viá»‡c, dá»± kiáº¿n sinh con vÃ o thÃ¡ng 8/2024. XIn há»i, tÃ´i cÃ³ Ä‘Æ°á»£c hÆ°á»Ÿng cháº¿ Ä‘á»™ thai sáº£n khÃ´ng?'],\n",
       " 'answers': ['Bá»™ Y táº¿ tráº£ lá»i váº¥n Ä‘á» nÃ y nhÆ° sau:NgÃ y 19/10/2023, ChÃ­nh phá»§ ban hÃ nh Nghá»‹ Ä‘á»‹nh sá»‘75/2023/NÄ-CPsá»­a Ä‘á»•i, bá»• sung má»™t sá»‘ Ä‘iá»u cá»§a Nghá»‹ Ä‘á»‹nh sá»‘ 146/2023/NÄ-CP ngÃ y 17/10/2018 cá»§a ChÃ­nh phá»§ quy Ä‘á»‹nh chi tiáº¿t vÃ  hÆ°á»›ng dáº«n biá»‡n phÃ¡p thi hÃ nh má»™t sá»‘ Ä‘iá»u cá»§a Luáº­t BHYT, trong Ä‘Ã³ cÃ³ quy Ä‘á»‹nh:NgÆ°á»i dÃ¢n tá»™c thiá»ƒu sá»‘ Ä‘ang sinh sá»‘ng táº¡i Ä‘á»‹a bÃ n cÃ¡c xÃ£ khu vá»±c II, khu vá»±c III, thÃ´n Ä‘áº·c biá»‡t khÃ³ khÄƒn thuá»™c vÃ¹ng Ä‘á»“ng bÃ o dÃ¢n tá»™c thiá»ƒu sá»‘ vÃ  miá»n nÃºi giai Ä‘oáº¡n 2016-2020 mÃ  cÃ¡c xÃ£ nÃ y khÃ´ng cÃ²n trong danh sÃ¡ch cÃ¡c xÃ£ khu vá»±c II, khu vá»±c III, thÃ´n Ä‘áº·c biá»‡t khÃ³ khÄƒn thuá»™c vÃ¹ng Ä‘á»“ng bÃ o dÃ¢n tá»™c thiá»ƒu sá»‘ vÃ  miá»n nÃºi giai Ä‘oáº¡n 2021-2025 theo quyáº¿t Ä‘á»‹nh cá»§a Thá»§ tÆ°á»›ng ChÃ­nh phá»§ tiáº¿p tá»¥c Ä‘Æ°á»£c ngÃ¢n sÃ¡ch NhÃ  nÆ°á»›c há»— trá»£ tá»‘i thiá»ƒu 70% má»©c Ä‘Ã³ng BHYT trong thá»i gian 36 thÃ¡ng ká»ƒ tá»« ngÃ y 1/11/2023.',\n",
       "  'Báº£o hiá»ƒm xÃ£ há»™i Viá»‡t Nam tráº£ lá»i váº¥n Ä‘á» nÃ y nhÆ° sau:Táº¡i Khoáº£n 2 Äiá»u 28Luáº­t BHXH nÄƒm 2006quy Ä‘á»‹nh Ä‘iá»u kiá»‡n hÆ°á»Ÿng cháº¿ Ä‘á»™ thai sáº£n Ä‘á»‘i vá»›i lao Ä‘á»™ng ná»¯ sinh con lÃ : Pháº£i Ä‘Ã³ng BHXH tá»« Ä‘á»§ 6 thÃ¡ng trá»Ÿ lÃªn trong thá»i gian 12 thÃ¡ng trÆ°á»›c khi sinh con.Äá»‘i chiáº¿u quy Ä‘á»‹nh nÃªu trÃªn, trÆ°á»ng há»£p cá»§a bÃ  NgÃ´ Äáº£m sinh con vÃ o thÃ¡ng 8/2014, trong thá»i gian 12 thÃ¡ng trÆ°á»›c khi sinh con (Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh tá»« thÃ¡ng 8/2013 Ä‘áº¿n thÃ¡ng 7/2014) bÃ  cÃ³ 12 thÃ¡ng Ä‘Ã³ng BHXH (náº¿u Ä‘Ã³ng BHXH liÃªn tá»¥c trong khoáº£ng thá»i gian nÃ y), bÃ  Ä‘á»§ Ä‘iá»u kiá»‡n hÆ°á»Ÿng cháº¿ Ä‘á»™ thai sáº£n khi sinh con.Äá» nghá»‹ bÃ  liÃªn há»‡ vá»›i cÆ¡ quan BHXH nÆ¡i Ä‘Æ¡n vá»‹ Ä‘Ã£ Ä‘Ã³ng BHXH Ä‘á»ƒ cÆ¡ quan BHXH hÆ°á»›ng dáº«n, tráº£ lá»i cá»¥ thá»ƒ vá» viá»‡c láº­p há»“ sÆ¡, giáº£i quyáº¿t cháº¿ Ä‘á»™ thai sáº£n Ä‘á»‘i vá»›i bÃ .',\n",
       "  'Báº£o hiá»ƒm xÃ£ há»™i Viá»‡t Nam tráº£ lá»i váº¥n Ä‘á» nÃ y nhÆ° sau:Táº¡i Khoáº£n 14 Äiá»u 1 ThÃ´ng tÆ° sá»‘18/2022/TT-BYTngÃ y 31/12/2022 cá»§a Bá»™ trÆ°á»Ÿng Bá»™ Y táº¿ sá»­a Ä‘á»•i, bá»• sung má»™t sá»‘ Ä‘iá»u cá»§a ThÃ´ng tÆ° sá»‘ 56/2017/TT-BYT ngÃ y 29/12/2017 cá»§a Bá»™ trÆ°á»Ÿng Bá»™ Y táº¿ quy Ä‘á»‹nh chi tiáº¿t thi hÃ nh Luáº­t BHXH vÃ  Luáº­t An toÃ n, vá»‡ sinh lao Ä‘á»™ng thuá»™c lÄ©nh y táº¿ quy Ä‘á»‹nh nhÆ° sau:TrÆ°á»ng há»£p ngÆ°á»i bá»‡nh cáº§n nghá»‰ Ä‘á»ƒ Ä‘iá»u trá»‹ ngoáº¡i trÃº sau khi ra viá»‡n thÃ¬ cÆ¡ quan BHXH cÄƒn cá»© sá»‘ ngÃ y nghá»‰ ghi táº¡i pháº§n ghi chÃº cá»§a giáº¥y ra viá»‡n Ä‘á»ƒ lÃ m cÄƒn cá»© thanh toÃ¡n cháº¿ Ä‘á»™ BHXH theo quy Ä‘á»‹nh.Äá»‘i chiáº¿u quy Ä‘á»‹nh nÃªu trÃªn, viá»‡c cÆ¡ sá»Ÿ khÃ¡m bá»‡nh, chá»¯a bá»‡nh ghi thÃªm sá»‘ ngÃ y nghá»‰ Ä‘á»ƒ Ä‘iá»u trá»‹ ngoáº¡i trÃº táº¡i pháº§n ghi chÃº trÃªn giáº¥y ra viá»‡n lÃ  Ä‘Ãºng quy Ä‘á»‹nh vÃ  sá»‘ ngÃ y nghá»‰ Ä‘iá»u trá»‹ ngoáº¡i trÃº Ä‘Ã³ sáº½ Ä‘Æ°á»£c cÆ¡ quan BHXH cÄƒn cá»© Ä‘á»ƒ giáº£i quyáº¿t cháº¿ Ä‘á»™ BHXH Ä‘á»‘i vá»›i bÃ  theo quy Ä‘á»‹nh.Do Ä‘Ã³, bÃ  khÃ´ng pháº£i xin cáº¥p thÃªm Giáº¥y chá»©ng nháº­n nghá»‰ viá»‡c hÆ°á»Ÿng BHXH Ä‘á»‘i vá»›i 10 ngÃ y nghá»‰ viá»‡c Ä‘iá»u trá»‹ ngoáº¡i trÃº nÃªu trÃªn.',\n",
       "  'Báº£o hiá»ƒm xÃ£ há»™i Viá»‡t Nam tráº£ lá»i váº¥n Ä‘á» nÃ y nhÆ° sau:Theo quy Ä‘á»‹nh cá»§a Luáº­t BHXH má»›i cÃ³ hiá»‡u lá»±c thi hÃ nh tá»« ngÃ y 1/7/2025, ngÆ°á»i tham gia BHXH tá»± nguyá»‡n Ä‘Æ°á»£c hÆ°á»Ÿng lÆ°Æ¡ng hÆ°u khi Ä‘á»§ tuá»•i nghá»‰ hÆ°u theo quy Ä‘á»‹nh táº¡i Khoáº£n 2 Äiá»u 169 cá»§a Bá»™ luáº­t Lao Ä‘á»™ng vÃ  cÃ³ thá»i gian Ä‘Ã³ng BHXH tá»« Ä‘á»§ 15 nÄƒm trá»Ÿ lÃªn.Theo quy Ä‘á»‹nh táº¡i Khoáº£n 2 Äiá»u 169 Bá»™ luáº­t Lao Ä‘á»™ng nÄƒm 2019 vÃ  Nghá»‹ Ä‘á»‹nh sá»‘ 135/2020/NÄ-CP ngÃ y 18/11/2020 cá»§a ChÃ­nh phá»§ thÃ¬ ká»ƒ tá»« ngÃ y 1/1/2021 tuá»•i nghá»‰ hÆ°u cá»§a lao Ä‘á»™ng ná»¯ lÃ  55 tuá»•i 4 thÃ¡ng; sau Ä‘Ã³, cá»© má»—i nÄƒm tÄƒng thÃªm 4 thÃ¡ng cho Ä‘áº¿n khi Ä‘á»§ 60 tuá»•i vÃ o nÄƒm 2035.TrÆ°á»ng há»£p cá»§a bÃ , nÄƒm nay 53 tuá»•i cÃ³ thá»i gian Ä‘Ã³ng BHXH lÃ  5 nÄƒm, náº¿u bÃ  Ä‘Ã³ng BHXH tá»± nguyá»‡n liÃªn tá»¥c Ä‘áº¿n khi 60 tuá»•i (nÄƒm 2031) thÃ¬ khi Ä‘Ã³ bÃ  cÃ³ thá»i gian Ä‘Ã³ng BHXH lÃ  12 nÄƒm chÆ°a Ä‘á»§ Ä‘iá»u kiá»‡n vá» thá»i gian Ä‘Ã³ng BHXH Ä‘á»ƒ hÆ°á»Ÿng cháº¿ Ä‘á»™ hÆ°u trÃ­.Do váº­y, bÃ  nÃªn tiáº¿p tá»¥c tham gia BHXH tá»± nguyá»‡n cho Ä‘áº¿n khi Ä‘á»§ 15 nÄƒm Ä‘Ã³ng BHXH Ä‘á»ƒ Ä‘Æ°á»£c hÆ°á»Ÿng lÆ°Æ¡ng hÆ°u háº±ng thÃ¡ng Ä‘áº¿n trá»n Ä‘á»i, khi Ä‘Ã³ bÃ  cÃ²n Ä‘Æ°á»£c cáº¥p tháº» BHYT miá»…n phÃ­ vÃ  Ä‘Æ°á»£c hÆ°á»Ÿng cÃ¡c cháº¿ Ä‘á»™ liÃªn quan cá»§a ngÆ°á»i hÆ°á»Ÿng lÆ°Æ¡ng hÆ°u.',\n",
       "  'Báº£o hiá»ƒm xÃ£ há»™i Viá»‡t Nam tráº£ lá»i váº¥n Ä‘á» nÃ y nhÆ° sau:Táº¡i Khoáº£n 2 vÃ  Khoáº£n 3 Äiá»u 31Luáº­t BHXH nÄƒm 2014quy Ä‘á»‹nh Ä‘iá»u kiá»‡n hÆ°á»Ÿng cháº¿ Ä‘á»™ thai sáº£n Ä‘á»‘i vá»›i lao Ä‘á»™ng ná»¯ sinh con lÃ : Pháº£i Ä‘Ã³ng BHXH tá»« Ä‘á»§ 6 thÃ¡ng trá»Ÿ lÃªn trong thá»i gian 12 thÃ¡ng trÆ°á»›c khi sinh con hoáº·c lao Ä‘á»™ng ná»¯ Ä‘Ã£ Ä‘Ã³ng BHXH tá»« Ä‘á»§ 12 thÃ¡ng trá»Ÿ lÃªn mÃ  khi mang thai pháº£i nghá»‰ viá»‡c Ä‘á»ƒ dÆ°á»¡ng thai theo chá»‰ Ä‘á»‹nh cá»§a cÆ¡ sá»Ÿ khÃ¡m bá»‡nh, chá»¯a bá»‡nh cÃ³ tháº©m quyá»n thÃ¬ pháº£i Ä‘Ã³ng BHXH tá»« Ä‘á»§ 3 thÃ¡ng trá»Ÿ lÃªn trong thá»i gian 12 thÃ¡ng trÆ°á»›c khi sinh con.Táº¡i Khoáº£n 4 Äiá»u 31 Luáº­t BHXH nÄƒm 2014 quy Ä‘á»‹nh: NgÆ°á»i lao Ä‘á»™ng Ä‘á»§ Ä‘iá»u kiá»‡n quy Ä‘á»‹nh táº¡i Khoáº£n 2 vÃ  Khoáº£n 3 Äiá»u nÃ y mÃ  cháº¥m dá»©t há»£p Ä‘á»“ng lao Ä‘á»™ng, há»£p Ä‘á»“ng lÃ m viá»‡c hoáº·c thÃ´i viá»‡c trÆ°á»›c thá»i Ä‘iá»ƒm sinh con váº«n Ä‘Æ°á»£c hÆ°á»Ÿng cháº¿ Ä‘á»™ thai sáº£n theo quy Ä‘á»‹nh.Táº¡i Khoáº£n 1 Äiá»u 9 ThÃ´ng tÆ° sá»‘ 59/2015/TT-BLÄTBXH ngÃ y 29/12/2015 cá»§a Bá»™ trÆ°á»Ÿng Bá»™ Lao Ä‘á»™ng - ThÆ°Æ¡ng binh vÃ  XÃ£ há»™i quy Ä‘á»‹nh, thá»i gian 12 thÃ¡ng trÆ°á»›c khi sinh con Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh nhÆ° sau:- TrÆ°á»ng há»£p sinh con trÆ°á»›c ngÃ y 15 cá»§a thÃ¡ng, thÃ¬ thÃ¡ng sinh con khÃ´ng tÃ­nh vÃ o thá»i gian 12 thÃ¡ng trÆ°á»›c khi sinh con;- TrÆ°á»ng há»£p sinh con tá»« ngÃ y 15 trá»Ÿ Ä‘i cá»§a thÃ¡ng vÃ  thÃ¡ng Ä‘Ã³ cÃ³ Ä‘Ã³ng BHXH, thÃ¬ thÃ¡ng sinh con Ä‘Æ°á»£c tÃ­nh vÃ o thá»i gian 12 thÃ¡ng trÆ°á»›c khi sinh con. TrÆ°á»ng há»£p thÃ¡ng Ä‘Ã³ khÃ´ng Ä‘Ã³ng BHXH thÃ¬ thá»±c hiá»‡n theo quy Ä‘á»‹nh táº¡i Äiá»ƒm a Khoáº£n nÃ y.TrÆ°á»ng há»£p cá»§a bÃ  Ä‘Ã£ nghá»‰ viá»‡c vÃ  dá»± kiáº¿n sinh con vÃ o thÃ¡ng 8/2024, trong thá»i gian 12 thÃ¡ng trÆ°á»›c khi sinh con (Ä‘Æ°á»£c tÃ­nh tá»« thÃ¡ng 8/2023 Ä‘áº¿n thÃ¡ng 7/2024), bÃ  cÃ³ 5 thÃ¡ng Ä‘Ã³ng BHXH (tá»« thÃ¡ng 8/2023 Ä‘áº¿n thÃ¡ng 12/2023). Do Ä‘Ã³, bÃ  khÃ´ng Ä‘á»§ Ä‘iá»u kiá»‡n hÆ°á»Ÿng cháº¿ Ä‘á»™ thai sáº£n khi sinh con theo quy Ä‘á»‹nh táº¡i Khoáº£n 2 Äiá»u 31 Luáº­t BHXH. TrÆ°á»ng há»£p bÃ  trong thá»i gian mang thai pháº£i nghá»‰ viá»‡c Ä‘á»ƒ dÆ°á»¡ng thai theo chá»‰ Ä‘á»‹nh cá»§a cÆ¡ sá»Ÿ khÃ¡m bá»‡nh, chá»¯a bá»‡nh cÃ³ tháº©m quyá»n thÃ¬ bÃ  Ä‘á»§ Ä‘iá»u kiá»‡n hÆ°á»Ÿng cháº¿ Ä‘á»™ thai sáº£n khi sinh con theo quy Ä‘á»‹nh táº¡i Khoáº£n 3 Äiá»u 31 Luáº­t BHXH nÃªu trÃªn.']}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PhobertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PhobertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.56 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 36274, 2573, 2292, 11674, 34423, 1913, 11058, 7360, 114, 2, 2, 8241, 11674, 34423, 1913, 14594, 2573, 17334, 4, 41832, 3069, 2573, 27481, 17837, 7221, 21353, 12868, 5276, 14453, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 4), (5, 7), (8, 11), (12, 15), (15, 19), (20, 22), (23, 27), (27, 29), (29, 30), (0, 0), (0, 0), (0, 3), (4, 7), (7, 11), (12, 14), (15, 21), (22, 24), (25, 30), (30, 31), (32, 35), (35, 37), (38, 40), (41, 44), (44, 46), (47, 50), (51, 54), (55, 59), (59, 62), (62, 64), (64, 65), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'start_positions': [18], 'end_positions': [18]}\n",
      "Dataset({\n",
      "    features: ['Question', 'context', 'answers', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "from datasets import Dataset\n",
    "\n",
    "# Äá»‹nh nghÄ©a hÃ m tokenize_function\n",
    "def tokenize_function(examples):\n",
    "    questions = examples['Question']\n",
    "    contexts = examples['context']\n",
    "    answers = examples['answers']\n",
    "    \n",
    "    encodings = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation='longest_first',\n",
    "        padding='max_length',\n",
    "        max_length=256,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, answer in enumerate(answers):\n",
    "        context = contexts[i]\n",
    "        start_char = context.find(answer)\n",
    "        end_char = start_char + len(answer)\n",
    "\n",
    "        if start_char == -1:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        offsets = encodings['offset_mapping'][i]\n",
    "        start_token = 0\n",
    "        end_token = 0\n",
    "\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= start_char < end:\n",
    "                start_token = idx\n",
    "            if start < end_char <= end:\n",
    "                end_token = idx\n",
    "\n",
    "        if end_token < start_token:\n",
    "            end_token = start_token\n",
    "        \n",
    "        start_positions.append(start_token)\n",
    "        end_positions.append(end_token)\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    print(encodings)\n",
    "    return encodings\n",
    "\n",
    "# Khá»Ÿi táº¡o tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# Dá»¯ liá»‡u máº«u\n",
    "data = {\n",
    "    'Question': [\"What is the capital of France?\"],\n",
    "    'context': [\"The capital of France is Paris, which is known for its landmarks.\"],\n",
    "    'answers': [\"Paris\"]\n",
    "}\n",
    "\n",
    "# Táº¡o dataset tá»« dá»¯ liá»‡u máº«u\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Ãp dá»¥ng hÃ m tokenize_function lÃªn dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# In káº¿t quáº£\n",
    "print(tokenized_datasets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12818/12818 [00:06<00:00, 1884.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 6/4809 [31:05<414:44:01, 310.86s/it]\n",
      "  0%|          | 0/4809 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 24\u001b[0m\n\u001b[0;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     18\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     19\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets,\n\u001b[0;32m     20\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m### 8. Train Model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2274\u001b[0m ):\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\trainer.py:3307\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3307\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3309\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3311\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\trainer.py:3338\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3337\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3338\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3339\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3340\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1497\u001b[0m, in \u001b[0;36mRobertaForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1497\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1509\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1511\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:832\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    823\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    825\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    826\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    827\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    830\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    831\u001b[0m )\n\u001b[1;32m--> 832\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    845\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:521\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    510\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    511\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    512\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m         output_attentions,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:452\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    449\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    450\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 452\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    457\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:465\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    464\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 465\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:376\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 376\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    378\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### 5. Load Model\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "### 6. Prepare Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "### 7. Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "### 8. Train Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning-lTTH8rYd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
